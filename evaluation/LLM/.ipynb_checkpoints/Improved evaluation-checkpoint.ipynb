{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46edb787-cf1a-4705-8e48-7ddc92fbad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating domain: camperplus ===\n",
      "â†’ Using 10 valid rounds for domain 'camperplus'\n",
      "âœ… Done: camperplus â†’ evaluation_summary_camperplus.csv (with 11th row average)\n",
      "\n",
      "=== Evaluating domain: supermarket ===\n",
      "âš ï¸ Predictions file not found for domain 'supermarket' â€” skipping.\n",
      "\n",
      "=== Evaluating domain: fish&chips ===\n",
      "âš ï¸ Predictions file not found for domain 'fish&chips' â€” skipping.\n",
      "\n",
      "=== Evaluating domain: planningpoker ===\n",
      "â†’ Using 10 valid rounds for domain 'planningpoker'\n",
      "âœ… Done: planningpoker â†’ evaluation_summary_planningpoker.csv (with 11th row average)\n",
      "\n",
      "=== Evaluating domain: grocery ===\n",
      "âš ï¸ Predictions file not found for domain 'grocery' â€” skipping.\n",
      "\n",
      "=== Evaluating domain: school ===\n",
      "âš ï¸ Predictions file not found for domain 'school' â€” skipping.\n",
      "\n",
      "=== Evaluating domain: sports ===\n",
      "âš ï¸ Predictions file not found for domain 'sports' â€” skipping.\n",
      "\n",
      "=== Evaluating domain: ticket ===\n",
      "âš ï¸ Predictions file not found for domain 'ticket' â€” skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_34408\\1101091889.py:19: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  ground_truth_all = ground_truth_all.applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_34408\\1101091889.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  synonyms_all = synonyms_all.applymap(lambda x: norm(x) if pd.notna(x) else \"\")\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_34408\\1101091889.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  notpunish_all = notpunish_all.applymap(lambda x: norm(x) if pd.notna(x) else \"\")\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_34408\\1101091889.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_34408\\1101091889.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === USER SETTINGS ===\n",
    "domains = [\"camperplus\", \"supermarket\", \"fish&chips\", \"planningpoker\", \"grocery\", \"school\", \"sports\", \"ticket\"]  # update this list with your actual domains\n",
    "base_predictions_pattern = \"predictions_{}.csv\"  # naming pattern for predictions files\n",
    "\n",
    "# === STEP 1: LOAD BASE FILES ===\n",
    "ground_truth_all = pd.read_csv(\"ground_truth.csv\")  # columns: domain, Class, Singular, Type\n",
    "synonyms_all = pd.read_csv(\"synonyms.csv\")          # columns: domain, concept, synonym_1, ...\n",
    "notpunish_all = pd.read_csv(\"notpunish.csv\")        # columns: domain, concept, synonym_1, ...\n",
    "\n",
    "# === STEP 2: NORMALIZE STRINGS ===\n",
    "def norm(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    return str(s).lower().strip()\n",
    "\n",
    "ground_truth_all = ground_truth_all.applymap(norm)\n",
    "synonyms_all = synonyms_all.applymap(lambda x: norm(x) if pd.notna(x) else \"\")\n",
    "notpunish_all = notpunish_all.applymap(lambda x: norm(x) if pd.notna(x) else \"\")\n",
    "\n",
    "# === STEP 3: LOOP OVER DOMAINS ===\n",
    "for domain in domains:\n",
    "    print(f\"\\n=== Evaluating domain: {domain} ===\")\n",
    "\n",
    "    # --- Load predictions file ---\n",
    "    pred_file = base_predictions_pattern.format(domain)\n",
    "    if not os.path.exists(pred_file):\n",
    "        print(f\"âš ï¸ Predictions file not found for domain '{domain}' â€” skipping.\")\n",
    "        continue\n",
    "    predictions = pd.read_csv(pred_file).applymap(norm)\n",
    "\n",
    "    # ðŸ§¹ Remove empty or all-NaN columns (fixes the 27-row issue)\n",
    "    predictions = predictions.dropna(axis=1, how='all')\n",
    "    predictions = predictions.loc[:, (predictions != \"\").any(axis=0)]\n",
    "    print(f\"â†’ Using {len(predictions.columns)} valid rounds for domain '{domain}'\")\n",
    "\n",
    "    # --- Filter for current domain ---\n",
    "    ground_truth = ground_truth_all[ground_truth_all['domain'] == domain]\n",
    "    synonyms = synonyms_all[synonyms_all['domain'] == domain]\n",
    "    notpunish = notpunish_all[notpunish_all['domain'] == domain]\n",
    "\n",
    "    if ground_truth.empty:\n",
    "        print(f\"âš ï¸ No ground truth found for domain '{domain}', skipping.\")\n",
    "        continue\n",
    "\n",
    "    # === STEP 4: BUILD SYNONYM GROUPS ===\n",
    "    synonym_groups = []\n",
    "    group_lookup = {}\n",
    "    concept_lookup = {}\n",
    "\n",
    "    for _, row in synonyms.iterrows():\n",
    "        concept = row['concept']\n",
    "        terms = [v for v in list(row[1:]) if v]\n",
    "        if concept not in terms:\n",
    "            terms.append(concept)\n",
    "        group = frozenset(terms)\n",
    "        synonym_groups.append((concept, group))\n",
    "        for t in group:\n",
    "            group_lookup[t] = group\n",
    "            concept_lookup[t] = concept\n",
    "\n",
    "    # === STEP 5: BUILD NOTPUNISH LOOKUP ===\n",
    "    notpunish_lookup = {}\n",
    "    for _, row in notpunish.iterrows():\n",
    "        concept = row['concept']\n",
    "        terms = [v for v in list(row[1:]) if v]\n",
    "        if concept not in terms:\n",
    "            terms.append(concept)\n",
    "        for t in terms:\n",
    "            notpunish_lookup[t] = concept\n",
    "\n",
    "    # === STEP 6: BUILD GROUND TRUTH GROUPS ===\n",
    "    gt_groups = []\n",
    "    gt_lookup = {}\n",
    "    type_lookup = {}\n",
    "    concepts_gt = set()\n",
    "\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        cls = row['Class']\n",
    "        singular = row['Singular']\n",
    "        type_value = row.get('Type', 'Must-have').lower()\n",
    "        concept = cls\n",
    "        group = frozenset([cls, singular])\n",
    "        gt_groups.append((concept, group))\n",
    "        type_lookup[concept] = type_value\n",
    "        concepts_gt.add(concept)\n",
    "        for t in group:\n",
    "            gt_lookup[t] = group\n",
    "            concept_lookup[t] = concept\n",
    "\n",
    "    # Add synonym groups as valid GT concepts\n",
    "    for concept, group in synonym_groups:\n",
    "        gt_groups.append((concept, group))\n",
    "        concepts_gt.add(concept)\n",
    "        for t in group:\n",
    "            gt_lookup[t] = group\n",
    "            concept_lookup[t] = concept\n",
    "\n",
    "    # === STEP 7: EVALUATE ROUNDS ===\n",
    "    results = []\n",
    "    annotated_columns = []\n",
    "\n",
    "    for col in predictions.columns:\n",
    "        preds_list = predictions[col].tolist()\n",
    "        tp_concepts = set()\n",
    "        fp_terms = set()\n",
    "        nopunish_terms = set()\n",
    "        used_concepts = set()\n",
    "        detected_terms = set(preds_list)\n",
    "        status_list = []\n",
    "\n",
    "        # --- Evaluate predictions ---\n",
    "        for term in preds_list:\n",
    "            if not term:\n",
    "                status_list.append(\"empty/na\")\n",
    "                continue\n",
    "\n",
    "            concept = concept_lookup.get(term)\n",
    "            group = gt_lookup.get(term)\n",
    "\n",
    "            if concept and group:\n",
    "                if concept not in used_concepts:\n",
    "                    tp_concepts.add(concept)\n",
    "                    used_concepts.add(concept)\n",
    "                    status_list.append(f\"TP (concept: {concept})\")\n",
    "                else:\n",
    "                    if term in notpunish_lookup:\n",
    "                        nopunish_terms.add(term)\n",
    "                        status_list.append(f\"NOPUNISH (redundant synonym of {concept})\")\n",
    "                    else:\n",
    "                        fp_terms.add(term)\n",
    "                        status_list.append(f\"FP (redundant synonym of {concept})\")\n",
    "            else:\n",
    "                if term in notpunish_lookup:\n",
    "                    nopunish_terms.add(term)\n",
    "                    status_list.append(\"NOPUNISH (not in GT but allowed)\")\n",
    "                else:\n",
    "                    fp_terms.add(term)\n",
    "                    status_list.append(\"FP (not in synonyms or GT)\")\n",
    "\n",
    "        # --- Compute FN & SHOULDHAVE ---\n",
    "        fn_concepts = set()\n",
    "        shouldhave_concepts = set()\n",
    "\n",
    "        for _, row in ground_truth.iterrows():\n",
    "            cls = row['Class']\n",
    "            singular = row['Singular']\n",
    "            concept = cls\n",
    "\n",
    "            variants = {cls, singular}\n",
    "            for v in list(variants):\n",
    "                if v in group_lookup:\n",
    "                    variants.update(group_lookup[v])\n",
    "\n",
    "            # check if all variants are missed\n",
    "            if not any(v in detected_terms for v in variants):\n",
    "                type_value = type_lookup.get(concept, 'must-have')\n",
    "                if type_value == 'must-have':\n",
    "                    fn_concepts.add(concept)\n",
    "                else:\n",
    "                    shouldhave_concepts.add(concept)\n",
    "\n",
    "        # --- Metrics ---\n",
    "        tp_count = len(tp_concepts)\n",
    "        fp_count = len(fp_terms)\n",
    "        fn_count = len(fn_concepts)\n",
    "\n",
    "        precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "        recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "\n",
    "        def fscore(beta):\n",
    "            return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            \"Domain\": domain,\n",
    "            \"Round\": col,\n",
    "            \"True_Positive\": tp_count,\n",
    "            \"False_Positive\": fp_count,\n",
    "            \"False_Negative\": fn_count,\n",
    "            \"SHOULDHAVE\": len(shouldhave_concepts),\n",
    "            \"NOPUNISH\": len(nopunish_terms),\n",
    "            \"Precision\": round(precision, 4),\n",
    "            \"Recall\": round(recall, 4),\n",
    "            \"F0.5\": round(fscore(0.5), 4),\n",
    "            \"F1\": round(fscore(1), 4),\n",
    "            \"F2\": round(fscore(2), 4),\n",
    "            \"Missed_Classes\": \", \".join(sorted(fn_concepts)),\n",
    "            \"Missed_ShouldHave\": \", \".join(sorted(shouldhave_concepts))\n",
    "        })\n",
    "\n",
    "        annotated_columns.append(pd.Series(preds_list, name=col))\n",
    "        annotated_columns.append(pd.Series(status_list, name=f\"{col}_status\"))\n",
    "\n",
    "    # === STEP 8: ADD AVERAGE ROW ===\n",
    "    summary = pd.DataFrame(results)\n",
    "\n",
    "    avg_row = {\n",
    "        \"Domain\": domain,\n",
    "        \"Round\": \"Average\",\n",
    "        \"True_Positive\": summary[\"True_Positive\"].mean(),\n",
    "        \"False_Positive\": summary[\"False_Positive\"].mean(),\n",
    "        \"False_Negative\": summary[\"False_Negative\"].mean(),\n",
    "        \"SHOULDHAVE\": summary[\"SHOULDHAVE\"].mean(),\n",
    "        \"NOPUNISH\": summary[\"NOPUNISH\"].mean(),\n",
    "        \"Precision\": summary[\"Precision\"].mean(),\n",
    "        \"Recall\": summary[\"Recall\"].mean(),\n",
    "        \"F0.5\": summary[\"F0.5\"].mean(),\n",
    "        \"F1\": summary[\"F1\"].mean(),\n",
    "        \"F2\": summary[\"F2\"].mean(),\n",
    "        \"Missed_Classes\": \"\",\n",
    "        \"Missed_ShouldHave\": \"\"\n",
    "    }\n",
    "    summary = pd.concat([summary, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "    # === STEP 9: SAVE OUTPUT FILES ===\n",
    "    annotated_predictions = pd.concat(annotated_columns, axis=1)\n",
    "    annotated_predictions.to_csv(f\"annotated_predictions_{domain}.csv\", index=False)\n",
    "    summary.to_csv(f\"evaluation_summary_{domain}.csv\", index=False)\n",
    "\n",
    "    print(f\"âœ… Done: {domain} â†’ evaluation_summary_{domain}.csv (with 11th row average)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf98e5-6721-4c72-b7f7-6c3ebd471318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
