{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbbc870",
   "metadata": {},
   "source": [
    "# Evaluation Script\n",
    "This script was used to evaluate the results from both the techniques to create the metrics as discussed in the paper. \n",
    "\n",
    "## Settings\n",
    "The script takes into account some assumptions in their calculations using the *should-have* and *must-have* classes. \n",
    "* Synonyms are allowed once, otherwise it becomes a **False Positive**\n",
    "* There is an exception for no-punishment in the gold&silver standard\n",
    "* If the class name is plurar, it also checks for the singular version. It does not do this the other way around (for example \"products\" -> \"product\", but not \"product\" -> \"products\")\n",
    "* **True Positives**: either in ground truth or in synonyms \n",
    "\n",
    "## Use\n",
    "When trying to use the script take notice that the directory as presented in the replication package is different from the way the script will behave. The results/LLM/predictions_*.csv files should be in the script folder. The ground_truth.csv, notpunished.csv and synonyms.csv should also be in this folder.\n",
    "\n",
    "## Use of LLM\n",
    "The script was produced using Chatgpt, it has been evaluated by the authors to make sure that no errors or misinterpretations exist.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420edf1-75e8-4727-ad63-b02305b98c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  ground_truth_all = ground_truth_all.applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  synonyms_all = synonyms_all.applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  notpunish_all = notpunish_all.applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating domain: camperplus ===\n",
      "→ Using 10 valid rounds for 'camperplus'\n",
      "\n",
      "=== Evaluating domain: supermarket ===\n",
      "→ Using 10 valid rounds for 'supermarket'\n",
      "\n",
      "=== Evaluating domain: fish&chips ===\n",
      "→ Using 10 valid rounds for 'fish&chips'\n",
      "\n",
      "=== Evaluating domain: planningpoker ===\n",
      "→ Using 10 valid rounds for 'planningpoker'\n",
      "\n",
      "=== Evaluating domain: grocery ===\n",
      "→ Using 10 valid rounds for 'grocery'\n",
      "\n",
      "=== Evaluating domain: school ===\n",
      "→ Using 10 valid rounds for 'school'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating domain: sports ===\n",
      "→ Using 10 valid rounds for 'sports'\n",
      "\n",
      "=== Evaluating domain: ticket ===\n",
      "→ Using 10 valid rounds for 'ticket'\n",
      "\n",
      "✅ overall_domain_summary.csv created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n",
      "C:\\Users\\aliek\\AppData\\Local\\Temp\\ipykernel_19664\\4187589452.py:49: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  predictions = pd.read_csv(pred_file).applymap(norm)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === USER SETTINGS ===\n",
    "domains = [\"camperplus\", \"supermarket\", \"fish&chips\", \"planningpoker\", \"grocery\", \"school\", \"sports\", \"ticket\"]\n",
    "base_predictions_pattern = \"predictions_{}.csv\"\n",
    "\n",
    "# === LOAD BASE FILES ===\n",
    "ground_truth_all = pd.read_csv(\"ground_truth.csv\")\n",
    "synonyms_all = pd.read_csv(\"synonyms.csv\")\n",
    "notpunish_all = pd.read_csv(\"notpunish.csv\")\n",
    "\n",
    "# === NORMALIZE COLUMN NAMES ===\n",
    "ground_truth_all.columns = ground_truth_all.columns.str.lower().str.strip()\n",
    "synonyms_all.columns = synonyms_all.columns.str.lower().str.strip()\n",
    "notpunish_all.columns = notpunish_all.columns.str.lower().str.strip()\n",
    "\n",
    "# === NORMALIZE STRINGS ===\n",
    "def norm(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    return str(s).lower().strip()\n",
    "\n",
    "ground_truth_all = ground_truth_all.applymap(norm)\n",
    "synonyms_all = synonyms_all.applymap(norm)\n",
    "notpunish_all = notpunish_all.applymap(norm)\n",
    "\n",
    "# === SAFE SAVE ===\n",
    "def safe_save(df, path):\n",
    "    try:\n",
    "        df.to_csv(path, index=False)\n",
    "    except PermissionError:\n",
    "        alt_path = path.replace(\".csv\", \"_new.csv\")\n",
    "        print(f\"⚠️ File locked: {path} — saving as {alt_path}\")\n",
    "        df.to_csv(alt_path, index=False)\n",
    "\n",
    "# === COLLECT OVERALL SUMMARY ===\n",
    "overall_rows = []\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for domain in domains:\n",
    "    print(f\"\\n=== Evaluating domain: {domain} ===\")\n",
    "\n",
    "    pred_file = base_predictions_pattern.format(domain)\n",
    "    if not os.path.exists(pred_file):\n",
    "        print(f\"⚠️ No predictions file for '{domain}', skipping.\")\n",
    "        continue\n",
    "\n",
    "    predictions = pd.read_csv(pred_file).applymap(norm)\n",
    "    predictions = predictions.dropna(axis=1, how='all')\n",
    "    predictions = predictions.loc[:, (predictions != \"\").any(axis=0)]\n",
    "    print(f\"→ Using {len(predictions.columns)} valid rounds for '{domain}'\")\n",
    "\n",
    "    # === Filter for current domain ===\n",
    "    ground_truth = ground_truth_all[ground_truth_all['domain'] == domain]\n",
    "    synonyms = synonyms_all[synonyms_all['domain'] == domain]\n",
    "    notpunish = notpunish_all[notpunish_all['domain'] == domain]\n",
    "\n",
    "    if ground_truth.empty and synonyms.empty:\n",
    "        print(f\"⚠️ No ground truth or synonyms for '{domain}', skipping.\")\n",
    "        continue\n",
    "\n",
    "    # === BUILD LOOKUPS ===\n",
    "    concept_lookup = {}\n",
    "    type_lookup = {}\n",
    "    group_lookup = {}\n",
    "\n",
    "    # --- Ground truth base ---\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        cls = row['class']\n",
    "        singular = row['singular']\n",
    "        ttype = row.get('type', 'must-have').lower()\n",
    "        for term in [cls, singular]:\n",
    "            if term:\n",
    "                concept_lookup[term] = cls\n",
    "        type_lookup[cls] = ttype\n",
    "\n",
    "    # --- Synonyms (includes Type column, cross-check with ground truth) ---\n",
    "    for _, row in synonyms.iterrows():\n",
    "        ttype_syn = row.get('type', 'must-have').lower()\n",
    "        concept = row['concept']\n",
    "        synonym_terms = [v for v in list(row[3:]) if v]\n",
    "        if concept not in synonym_terms:\n",
    "            synonym_terms.append(concept)\n",
    "\n",
    "        # Check consistency with ground truth\n",
    "        gt_type = type_lookup.get(concept)\n",
    "        if gt_type and gt_type != ttype_syn:\n",
    "            print(f\"⚠️ Type mismatch for concept '{concept}' in domain '{domain}': \"\n",
    "                  f\"Ground truth = '{gt_type}', Synonyms file = '{ttype_syn}'. Using ground truth.\")\n",
    "            ttype_syn = gt_type  # enforce ground truth\n",
    "\n",
    "        group = set(synonym_terms)\n",
    "        for term in group:\n",
    "            group_lookup[term] = group\n",
    "            concept_lookup[term] = concept\n",
    "        type_lookup[concept] = ttype_syn\n",
    "\n",
    "    # --- Not punish lookup ---\n",
    "    notpunish_lookup = {}\n",
    "    for _, row in notpunish.iterrows():\n",
    "        concept = row['concept']\n",
    "        terms = [v for v in list(row[1:]) if v]\n",
    "        if concept not in terms:\n",
    "            terms.append(concept)\n",
    "        for t in terms:\n",
    "            notpunish_lookup[t] = concept\n",
    "\n",
    "    # === EVALUATE ROUNDS ===\n",
    "    results = []\n",
    "    annotated_columns = []\n",
    "\n",
    "    for col in predictions.columns:\n",
    "        preds = predictions[col].tolist()\n",
    "        detected_terms = set(preds)\n",
    "        status_list = []\n",
    "\n",
    "        tp_concepts = set()\n",
    "        fp_terms = set()\n",
    "        nopunish_terms = set()\n",
    "        used_concepts = set()\n",
    "\n",
    "        # --- Evaluate TPs and FPs ---\n",
    "        for term in preds:\n",
    "            if not term:\n",
    "                status_list.append(\"empty/na\")\n",
    "                continue\n",
    "\n",
    "            canonical = concept_lookup.get(term)\n",
    "            if canonical:\n",
    "                canonical_type = type_lookup.get(canonical, 'must-have')\n",
    "                if canonical not in used_concepts:\n",
    "                    used_concepts.add(canonical)\n",
    "                    tp_concepts.add(canonical)\n",
    "                    # Use the correct type label from type_lookup\n",
    "                    if canonical_type == 'must-have':\n",
    "                        status_list.append(f\"TP (M: {canonical})\")\n",
    "                    else:\n",
    "                        status_list.append(f\"TP (S: {canonical})\")\n",
    "                else:\n",
    "                    # Repeated concept\n",
    "                    if term in notpunish_lookup:\n",
    "                        nopunish_terms.add(term)\n",
    "                        status_list.append(\"NOPUNISH\")\n",
    "                    else:\n",
    "                        fp_terms.add(term)\n",
    "                        status_list.append(\"FP (redundant synonym)\")\n",
    "            else:\n",
    "                if term in notpunish_lookup:\n",
    "                    nopunish_terms.add(term)\n",
    "                    status_list.append(\"NOPUNISH\")\n",
    "                else:\n",
    "                    fp_terms.add(term)\n",
    "                    status_list.append(\"FP (unknown term)\")\n",
    "\n",
    "        # --- Compute FNs ---\n",
    "        fn_m, fn_s = set(), set()\n",
    "        for _, row in ground_truth.iterrows():\n",
    "            cls = row['class']\n",
    "            singular = row['singular']\n",
    "            ttype = row.get('type', 'must-have').lower()\n",
    "            variants = {cls, singular}\n",
    "            for v in list(variants):\n",
    "                if v in group_lookup:\n",
    "                    variants.update(group_lookup[v])\n",
    "            if not any(v in detected_terms for v in variants):\n",
    "                if ttype == 'must-have':\n",
    "                    fn_m.add(cls)\n",
    "                else:\n",
    "                    fn_s.add(cls)\n",
    "\n",
    "        # --- Metrics ---\n",
    "        tp_m = sum(1 for c in tp_concepts if type_lookup.get(c) == 'must-have')\n",
    "        tp_s = sum(1 for c in tp_concepts if type_lookup.get(c) == 'should-have')\n",
    "        fp_count = len(fp_terms)\n",
    "        nopunish_count = len(nopunish_terms)\n",
    "        fn_m_count, fn_s_count = len(fn_m), len(fn_s)\n",
    "        tp_total, fn_total = tp_m + tp_s, fn_m_count + fn_s_count\n",
    "\n",
    "        def safe_div(a, b): return a / b if b else 0\n",
    "        def fscore(p, r, beta): return (1 + beta**2) * (p * r) / ((beta**2 * p) + r) if (p + r) > 0 else 0\n",
    "\n",
    "        # overall\n",
    "        precision = safe_div(tp_total, tp_total + fp_count)\n",
    "        recall = safe_div(tp_total, tp_total + fn_total)\n",
    "\n",
    "        # must-have\n",
    "        precision_m = safe_div(tp_m, tp_m + fp_count)\n",
    "        recall_m = safe_div(tp_m, tp_m + fn_m_count)\n",
    "\n",
    "        # should-have\n",
    "        precision_s = safe_div(tp_s, tp_s + fp_count)\n",
    "        recall_s = safe_div(tp_s, tp_s + fn_s_count)\n",
    "\n",
    "        # --- Add results ---\n",
    "        results.append({\n",
    "            \"Domain\": domain,\n",
    "            \"Round\": col,\n",
    "            \"TP (M)\": tp_m,\n",
    "            \"TP (S)\": tp_s,\n",
    "            \"FP\": fp_count,\n",
    "            \"FN (M)\": fn_m_count,\n",
    "            \"FN (S)\": fn_s_count,\n",
    "            \"NOPUNISH\": nopunish_count,\n",
    "            \"PRECISION (overall)\": precision,\n",
    "            \"RECALL (overall)\": recall,\n",
    "            \"F0.5 (overall)\": fscore(precision, recall, 0.5),\n",
    "            \"F1 (overall)\": fscore(precision, recall, 1),\n",
    "            \"F2 (overall)\": fscore(precision, recall, 2),\n",
    "            \"PRECISION (M)\": precision_m,\n",
    "            \"RECALL (M)\": recall_m,\n",
    "            \"F0.5 (M)\": fscore(precision_m, recall_m, 0.5),\n",
    "            \"F1 (M)\": fscore(precision_m, recall_m, 1),\n",
    "            \"F2 (M)\": fscore(precision_m, recall_m, 2),\n",
    "            \"PRECISION (S)\": precision_s,\n",
    "            \"RECALL (S)\": recall_s,\n",
    "            \"F0.5 (S)\": fscore(precision_s, recall_s, 0.5),\n",
    "            \"F1 (S)\": fscore(precision_s, recall_s, 1),\n",
    "            \"F2 (S)\": fscore(precision_s, recall_s, 2),\n",
    "            \"Missed_Classes\": \", \".join(sorted(fn_m)),\n",
    "            \"Missed_ShouldHave\": \", \".join(sorted(fn_s))\n",
    "        })\n",
    "\n",
    "        annotated_columns.append(pd.Series(preds, name=col))\n",
    "        annotated_columns.append(pd.Series(status_list, name=f\"{col}_status\"))\n",
    "\n",
    "    # === DOMAIN SUMMARY (with Average row) ===\n",
    "    summary = pd.DataFrame(results)\n",
    "    avg_row = {col: summary[col].mean() if pd.api.types.is_numeric_dtype(summary[col]) else \"\" for col in summary.columns}\n",
    "    avg_row[\"Domain\"] = domain\n",
    "    avg_row[\"Round\"] = \"Average\"\n",
    "    summary = pd.concat([summary, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "    # === SAVE DOMAIN FILES ===\n",
    "    annotated_predictions = pd.concat(annotated_columns, axis=1)\n",
    "    safe_save(annotated_predictions, f\"annotated_predictions_{domain}.csv\")\n",
    "    safe_save(summary, f\"evaluation_summary_{domain}.csv\")\n",
    "\n",
    "    # === ADD DOMAIN AVG TO OVERALL SUMMARY ===\n",
    "    overall_rows.append(avg_row)\n",
    "\n",
    "# === BUILD OVERALL DOMAIN SUMMARY ===\n",
    "if overall_rows:\n",
    "    overall_summary = pd.DataFrame(overall_rows)\n",
    "    avg_row_all = {col: overall_summary[col].mean() if pd.api.types.is_numeric_dtype(overall_summary[col]) else \"\" for col in overall_summary.columns}\n",
    "    avg_row_all[\"Domain\"] = \"OVERALL_AVERAGE\"\n",
    "    overall_summary = pd.concat([overall_summary, pd.DataFrame([avg_row_all])], ignore_index=True)\n",
    "    safe_save(overall_summary, \"overall_domain_summary.csv\")\n",
    "    print(\"\\n✅ overall_domain_summary.csv created.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No domain summaries to aggregate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8155fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ab3b6-052e-41c6-b97e-88c54700ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
